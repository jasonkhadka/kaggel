{"cells":[{"metadata":{},"cell_type":"markdown","source":"1. # Task : To identify if an image shows a healthy cassava leaf or does it shoe an unhealthy leaf with particular disease\nInput: 21397 RGB images with 800x600 resolution\n\n# Here: Run a single long run for a single lr to see how it converges\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, sys\n\n\nDATA_DIR = \"../input/cassava-leaf-disease-classification/\"\nos.listdir(DATA_DIR)\n\npytorch_image_model_path = \"../input/pytorchimagemodels/pytorch-image-models-master\"\n\nsys.path.append(pytorch_image_model_path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from PIL import Image\nimport torch\nimport torchvision\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport timm\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport json\n\nimport time\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.backends.cudnn.enabled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configs"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label = DATA_DIR+\"train.csv\"\nlabel_mapping = DATA_DIR+\"label_num_to_disease_map.json\"\ntrain_images = DATA_DIR+\"train_images/\"\nvalidate_images = DATA_DIR+\"test_images/\"\npredicted_label = DATA_DIR+\"predictions.csv\"\n\nkaggle_model_path = \"/kaggle/working/model_%s_fold_%d_epoch_%d.pth\"\nloss_figure = \"/kaggle/working/loss_figure.png\"\n\ninput_size = 512\nseed = 331\nbatch_size_train = 16\nbatch_size_test = 32\nnum_workers = 4\ntest_data_size = 0.02#0.9\n\nnum_epoch = 30\nnum_of_folds = 5\naccum_iter = 10\n\nlr = 5e-5\nweight_decay = 1e-6\n\nscheduler_step_size = 10\n\n\nmodel_architecture = 'tf_efficientnet_b3_ns'\n#model_architecture = 'tf_efficientnet_b0_ns'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the data\n## 1. Labels on the files\n    > Looks like data is imbalaced with class 3 being dominant"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(train_label)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.1 For tuning, smaller sample size is used. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"label\"].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation images : There is only 1 validation image given. Rest will be shown only after submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_images = os.listdir(validate_images)\nprint(validation_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10,10))\nfor idx, image_file in enumerate(validation_images):\n    ax = fig.add_subplot(1, 1, idx+1)\n    ax.imshow(Image.open(validate_images+image_file))\n    ax.set_xticks([])\n    ax.set_yticks([])\nfig.tight_layout()\nfig.suptitle(\"Validation Image\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Mapping of the labels to disease name\n> So, '3': 'Cassava Mosaic Disease (CMD)' is the dominant one"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(label_mapping) as f:\n    label_dict = json.load(f)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Looking at the examples of the diseases"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.iloc[0].image_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image_file in df_train.sample(100)[\"image_id\"]:\n    print(Image.open(train_images+image_file).size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_images = 4\n\nfigsize = 4\nfor target_label in range(5):\n    sampled_images =  df_train[df_train[\"label\"] == target_label].sample(num_images)[\"image_id\"].to_list()\n    fig = plt.figure(figsize = (num_images*figsize, figsize))\n\n    for idx, image_file in enumerate(sampled_images):\n        ax = fig.add_subplot(1, num_images, idx+1)\n        ax.imshow(Image.open(train_images+image_file))\n        ax.set_xticks([])\n        ax.set_yticks([])\n    fig.tight_layout()\n    fig.suptitle(label_dict[str(target_label)])\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparation of Dataset\n### Train/Validation image augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return Compose([\n            RandomResizedCrop(input_size, input_size),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            CoarseDropout(p=0.5),\n            Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            CenterCrop(input_size, input_size, p=1.),\n            Resize(input_size,input_size),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CassavaDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, df, data_dir, transformations = None, output_labels = True):\n        \"\"\"\n            df: dataframe\n            data_dir: directory where images are stored\n            transformations: transformations applied to images\n            output_labels: to output labels or not \n        \"\"\"\n        super().__init__()\n        self.df = df.copy()\n        self.transformations = transformations\n        self.data_dir = data_dir\n        self.output_labels = output_labels\n        if self.output_labels:\n            self.labels = df[\"label\"].values\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        img = np.asarray(Image.open(self.data_dir+self.df.loc[index][\"image_id\"]))\n        if self.transformations:\n            img = self.transformations(image = img)[\"image\"]\n        if self.output_labels:\n            return img, self.labels[index]\n        else:\n            return img\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dataloader(df, train_idx, test_idx, data_dir, transformations):\n    \"\"\"\n    df: total input dataset containg image filename and target labels\n    train_idx: indices for train data\n    test_idx: indices for test data\n    data_dir: directory for the images\n    \"\"\"\n    \n    df_train = df.loc[train_idx].reset_index(drop=True)\n    df_test = df.loc[test_idx].reset_index(drop=True)\n    \n    train_dataset = CassavaDataset(df_train, data_dir, transformations, output_labels = True)\n    test_dataset = CassavaDataset(df_test, data_dir, transformations, output_labels = True)\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset, \n                                               batch_size = batch_size_train,\n                                               pin_memory = False, \n                                               shuffle = True,\n                                               num_workers = num_workers\n                                               )\n    test_loader = torch.utils.data.DataLoader(test_dataset, \n                                               batch_size = batch_size_test,\n                                               pin_memory = False, \n                                               shuffle = True,\n                                               num_workers = num_workers\n                                               )\n    \n    return train_loader, test_loader\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Designing the model and supplementary functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CassavaClassifier(nn.Module):\n    def __init__(self, model_architecture, n_class, pretrained = False):\n        super().__init__()\n        self.model = timm.create_model(model_architecture, pretrained = pretrained)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, n_class)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_one_epoch(epoch, model, loss_func, optimizer, train_loader, device, \n                    scheduler = None, scheduler_update = False, loss_array = None):\n    \n    \n    progressbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    epoch_loss = 0\n    sample_num = 0\n    \n    for step, (images, image_labels) in progressbar:\n        images = images.to(device).float()\n        image_labels = image_labels.to(device).long()\n        \n        with autocast():\n            image_predictions = model(images)\n            \n            loss = loss_func(image_predictions, image_labels)\n            loss = loss/accum_iter\n        \n        scaler.scale(loss).backward()\n        \n        epoch_loss += loss.item()*image_labels.shape[0]\n        sample_num += image_labels.shape[0]\n        \n        if ((step+1)% accum_iter == 0) or ((step+1) == len(train_loader)):\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        progressbar.set_description(f\"epoch {epoch} \\tloss:{loss.item():.4f} \\t lr:{scheduler.get_last_lr()}\")\n        \n    if scheduler is not None and scheduler_update:\n        scheduler.step()\n    \n    epoch_loss /= sample_num\n    \n    print(f\"Epoch {epoch} \\tloss {epoch_loss}\")\n    \n    if loss_array is not None:\n        loss_array.append(epoch_loss)\n        \n\n            \n\ndef test_one_epoch(epoch, model, loss_func, test_loader, device, \n                  scheduler = None, scheduler_update = False, loss_array = None):\n    \n    # evaluate model\n    model.eval()\n    \n    loss_sum = 0\n    total_images = 0\n    total_correct = 0\n    \n    progressbar = tqdm(enumerate(test_loader), total=len(test_loader))\n    \n    for step, (images, image_labels) in progressbar:\n        images = images.to(device).float()\n        image_labels = image_labels.to(device).long()\n        \n        image_preds = model(images)\n        \n        loss = loss_func(image_preds, image_labels)\n        \n        image_preds = image_preds.detach().cpu().data.max(1, keepdim=True)[1]\n        image_labels = image_labels.detach().cpu()\n        \n        correct = image_preds.eq(image_labels.view_as(image_preds)).sum()\n        \n        loss_sum += loss\n        total_images += image_labels.shape[0]\n        total_correct += correct\n        \n        progressbar.set_description(f\"Test epoch {epoch} \\tloss:{loss:.4f} \\t accuracy:{correct/image_labels.shape[0]}\")\n        \n        images.detach().cpu()\n        image_labels.detach().cpu()\n        \n    \n    print(\"Test accuracy = {:.4f} \\t  {:d}/{:d}\".format(total_correct/total_images,\n                                                                total_correct,\n                                                                total_images))\n        \n\n\ndef initialize_network(device, learning_rate):\n    model = CassavaClassifier(model_architecture, df_train.label.nunique(), pretrained = True).to(device)\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n    scaler = GradScaler()\n    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=0.1,last_epoch=-1)\n    \n    return model, optimizer, scaler, exp_lr_scheduler\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\ndef fold_progressbar(fold, num_of_fold, charlen = 20):\n    fold += 1\n    print(f\"Fold :|{'#'*int(fold/num_of_fold*charlen)}{' '*int((1-fold/num_of_fold)*charlen)}| \\t{fold}/{num_of_fold}\")\n\n\ndef plot_loss(fold_loss_dict):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    for fold, loss_val in fold_loss_dict.items():\n        ax.plot(loss_val, label = fold)\n\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Loss\")\n    fig.legend()\n    fig.tight_layout()\n    fig.savefig(loss_figure)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedShuffleSplit(n_splits = num_of_folds, random_state = seed,\n                              test_size = test_data_size).split(\n            np.zeros(df_train.shape[0]), df_train.label.values)\n\nscheduler_update = True\nfold_loss_dict = {}\n\nfor fold, (train_idx, test_idx) in enumerate(folds):\n    \n    set_seed(seed+fold)\n    \n    fold_progressbar(fold, num_of_folds)\n    \n    #lr = learning_rate_array[fold]\n    \n    model, optimizer, scaler, scheduler = initialize_network(device, learning_rate = lr)\n    \n    # get the data loaders\n    train_loader, test_loader = get_dataloader(df_train, train_idx, test_idx, train_images, \n                                               get_train_transforms())\n    \n    loss_func = nn.CrossEntropyLoss().to(device)\n    loss_test = nn.CrossEntropyLoss().to(device)\n    \n    loss_array = []\n    \n    for epoch in range(num_epoch):\n        \n        train_one_epoch(epoch, model, loss_func, optimizer, train_loader, device, \n                        scheduler, \n                        scheduler_update, loss_array = loss_array)\n        \n        if (epoch)%5 == 0:\n            with torch.no_grad():\n                test_one_epoch(epoch, model, loss_test, test_loader, device, \n                               scheduler, \n                               False)\n        torch.save(model.state_dict(), kaggle_model_path%(model_architecture,\n                                                                       fold, \n                                                                       epoch))\n        fold_loss_dict[fold] = loss_array\n    \n    if fold+1 >= num_of_folds:\n        del optimizer, train_loader, test_loader, scaler, scheduler\n    else:\n        del model, optimizer, train_loader, test_loader, scaler, scheduler\n    torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss(fold_loss_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# making validation dataset\ndf_validation = pd.DataFrame()\ndf_validation[\"image_id\"] = list(os.listdir(validate_images)) \n\nvalidation_dataset = CassavaDataset(df_validation, validate_images,\n                                    transformations = get_train_transforms(),\n                                    output_labels = False\n                                    )\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, \n                                               batch_size = batch_size_test,\n                                               num_workers = num_workers,\n                                                shuffle = False, \n                                                pin_memory = False\n                                               )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_predictions = []\nfor valid_batch_id, validation_tensor in enumerate(validation_loader):\n    validation_tensor = validation_tensor.to(device)\n    image_preds = model(validation_tensor)\n    image_preds = image_preds.detach().cpu().data.max(1)[1].tolist()\n    valid_predictions += image_preds\n\ndf_validation[\"label\"] = valid_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_validation.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}